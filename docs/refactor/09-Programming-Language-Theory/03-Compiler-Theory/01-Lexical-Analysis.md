# 01-è¯æ³•åˆ†æ (Lexical Analysis)

## ç›®å½•

- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. æ­£åˆ™è¡¨è¾¾å¼](#2-æ­£åˆ™è¡¨è¾¾å¼)
- [3. æœ‰é™è‡ªåŠ¨æœº](#3-æœ‰é™è‡ªåŠ¨æœº)
- [4. è¯æ³•åˆ†æå™¨](#4-è¯æ³•åˆ†æå™¨)
- [5. Goè¯­è¨€å®ç°](#5-goè¯­è¨€å®ç°)
- [6. åº”ç”¨å®ä¾‹](#6-åº”ç”¨å®ä¾‹)

## 1. æ¦‚è¿°

### 1.1 è¯æ³•åˆ†æå®šä¹‰

è¯æ³•åˆ†ææ˜¯å°†æºä»£ç å­—ç¬¦æµè½¬æ¢ä¸ºè¯æ³•å•å…ƒï¼ˆtokenï¼‰åºåˆ—çš„è¿‡ç¨‹ã€‚

**å½¢å¼åŒ–å®šä¹‰**ï¼š

```latex
\text{è¯æ³•åˆ†æå™¨} = (\Sigma, Q, \delta, q_0, F, T)
```

å…¶ä¸­ï¼š

- $\Sigma$ æ˜¯è¾“å…¥å­—æ¯è¡¨
- $Q$ æ˜¯çŠ¶æ€é›†åˆ
- $\delta$ æ˜¯è½¬ç§»å‡½æ•°
- $q_0$ æ˜¯åˆå§‹çŠ¶æ€
- $F$ æ˜¯æ¥å—çŠ¶æ€é›†åˆ
- $T$ æ˜¯è¯æ³•å•å…ƒç±»å‹é›†åˆ

### 1.2 æ ¸å¿ƒæ¦‚å¿µ

#### 1.2.1 è¯æ³•å•å…ƒ

```go
// Token è¯æ³•å•å…ƒ
type Token struct {
    Type    TokenType
    Value   string
    Line    int
    Column  int
}

type TokenType int

const (
    TokenEOF TokenType = iota
    TokenIdentifier
    TokenNumber
    TokenString
    TokenKeyword
    TokenOperator
    TokenDelimiter
    TokenComment
)
```

## 2. æ­£åˆ™è¡¨è¾¾å¼

### 2.1 æ­£åˆ™è¡¨è¾¾å¼å®šä¹‰

```latex
R ::= \epsilon \mid a \mid R_1 \cdot R_2 \mid R_1 + R_2 \mid R^*
```

**Goè¯­è¨€å®ç°**ï¼š

```go
// Regex æ­£åˆ™è¡¨è¾¾å¼æ¥å£
type Regex interface {
    Match(input string) bool
    String() string
}

// EmptyRegex ç©ºæ­£åˆ™è¡¨è¾¾å¼
type EmptyRegex struct{}

func (er *EmptyRegex) Match(input string) bool {
    return input == ""
}

func (er *EmptyRegex) String() string {
    return "Îµ"
}

// CharRegex å­—ç¬¦æ­£åˆ™è¡¨è¾¾å¼
type CharRegex struct {
    Char rune
}

func (cr *CharRegex) Match(input string) bool {
    return len(input) == 1 && rune(input[0]) == cr.Char
}

func (cr *CharRegex) String() string {
    return string(cr.Char)
}

// ConcatRegex è¿æ¥æ­£åˆ™è¡¨è¾¾å¼
type ConcatRegex struct {
    Left, Right Regex
}

func (concr *ConcatRegex) Match(input string) bool {
    for i := 0; i <= len(input); i++ {
        if concr.Left.Match(input[:i]) && concr.Right.Match(input[i:]) {
            return true
        }
    }
    return false
}

func (concr *ConcatRegex) String() string {
    return fmt.Sprintf("(%sÂ·%s)", concr.Left, concr.Right)
}

// UnionRegex å¹¶é›†æ­£åˆ™è¡¨è¾¾å¼
type UnionRegex struct {
    Left, Right Regex
}

func (ur *UnionRegex) Match(input string) bool {
    return ur.Left.Match(input) || ur.Right.Match(input)
}

func (ur *UnionRegex) String() string {
    return fmt.Sprintf("(%s+%s)", ur.Left, ur.Right)
}

// StarRegex æ˜Ÿå·æ­£åˆ™è¡¨è¾¾å¼
type StarRegex struct {
    Inner Regex
}

func (sr *StarRegex) Match(input string) bool {
    if input == "" {
        return true
    }
    
    for i := 1; i <= len(input); i++ {
        if sr.Inner.Match(input[:i]) && sr.Match(input[i:]) {
            return true
        }
    }
    return false
}

func (sr *StarRegex) String() string {
    return fmt.Sprintf("(%s)*", sr.Inner)
}
```

## 3. æœ‰é™è‡ªåŠ¨æœº

### 3.1 ç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº (DFA)

```latex
\text{DFA} = (Q, \Sigma, \delta, q_0, F)
```

**Goè¯­è¨€å®ç°**ï¼š

```go
// DFA ç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº
type DFA struct {
    States     map[string]bool
    Alphabet   map[rune]bool
    Transitions map[string]map[rune]string
    StartState string
    AcceptStates map[string]bool
}

func (dfa *DFA) Accept(input string) bool {
    currentState := dfa.StartState
    
    for _, char := range input {
        if !dfa.Alphabet[char] {
            return false
        }
        
        nextState, exists := dfa.Transitions[currentState][char]
        if !exists {
            return false
        }
        
        currentState = nextState
    }
    
    return dfa.AcceptStates[currentState]
}

func (dfa *DFA) AddTransition(from, to string, char rune) {
    if dfa.Transitions[from] == nil {
        dfa.Transitions[from] = make(map[rune]string)
    }
    dfa.Transitions[from][char] = to
    dfa.Alphabet[char] = true
}
```

### 3.2 éç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº (NFA)

```go
// NFA éç¡®å®šæ€§æœ‰é™è‡ªåŠ¨æœº
type NFA struct {
    States     map[string]bool
    Alphabet   map[rune]bool
    Transitions map[string]map[rune][]string
    StartState string
    AcceptStates map[string]bool
}

func (nfa *NFA) Accept(input string) bool {
    currentStates := map[string]bool{nfa.StartState: true}
    
    for _, char := range input {
        nextStates := make(map[string]bool)
        
        for state := range currentStates {
            if transitions, exists := nfa.Transitions[state][char]; exists {
                for _, nextState := range transitions {
                    nextStates[nextState] = true
                }
            }
        }
        
        if len(nextStates) == 0 {
            return false
        }
        
        currentStates = nextStates
    }
    
    // æ£€æŸ¥æ˜¯å¦æœ‰æ¥å—çŠ¶æ€
    for state := range currentStates {
        if nfa.AcceptStates[state] {
            return true
        }
    }
    
    return false
}
```

## 4. è¯æ³•åˆ†æå™¨

### 4.1 è¯æ³•åˆ†æå™¨å®ç°

```go
// Lexer è¯æ³•åˆ†æå™¨
type Lexer struct {
    Input   string
    Position int
    Line    int
    Column  int
    Tokens  []Token
}

func (l *Lexer) Tokenize() []Token {
    l.Tokens = []Token{}
    
    for l.Position < len(l.Input) {
        // è·³è¿‡ç©ºç™½å­—ç¬¦
        l.skipWhitespace()
        
        if l.Position >= len(l.Input) {
            break
        }
        
        // è¯†åˆ«è¯æ³•å•å…ƒ
        token := l.nextToken()
        if token.Type != TokenEOF {
            l.Tokens = append(l.Tokens, token)
        }
    }
    
    // æ·»åŠ EOFæ ‡è®°
    l.Tokens = append(l.Tokens, Token{
        Type: TokenEOF,
        Value: "",
        Line: l.Line,
        Column: l.Column,
    })
    
    return l.Tokens
}

func (l *Lexer) skipWhitespace() {
    for l.Position < len(l.Input) {
        char := rune(l.Input[l.Position])
        if char == ' ' || char == '\t' {
            l.Position++
            l.Column++
        } else if char == '\n' {
            l.Position++
            l.Line++
            l.Column = 1
        } else {
            break
        }
    }
}

func (l *Lexer) nextToken() Token {
    char := rune(l.Input[l.Position])
    startPos := l.Position
    startLine := l.Line
    startColumn := l.Column
    
    // è¯†åˆ«æ ‡è¯†ç¬¦æˆ–å…³é”®å­—
    if isLetter(char) {
        return l.readIdentifier()
    }
    
    // è¯†åˆ«æ•°å­—
    if isDigit(char) {
        return l.readNumber()
    }
    
    // è¯†åˆ«å­—ç¬¦ä¸²
    if char == '"' {
        return l.readString()
    }
    
    // è¯†åˆ«æ“ä½œç¬¦
    if isOperator(char) {
        return l.readOperator()
    }
    
    // è¯†åˆ«åˆ†éš”ç¬¦
    if isDelimiter(char) {
        return l.readDelimiter()
    }
    
    // æœªçŸ¥å­—ç¬¦
    l.Position++
    l.Column++
    return Token{
        Type: TokenEOF,
        Value: string(char),
        Line: startLine,
        Column: startColumn,
    }
}

func (l *Lexer) readIdentifier() Token {
    startPos := l.Position
    startLine := l.Line
    startColumn := l.Column
    
    for l.Position < len(l.Input) {
        char := rune(l.Input[l.Position])
        if !isLetter(char) && !isDigit(char) && char != '_' {
            break
        }
        l.Position++
        l.Column++
    }
    
    value := l.Input[startPos:l.Position]
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºå…³é”®å­—
    if isKeyword(value) {
        return Token{
            Type: TokenKeyword,
            Value: value,
            Line: startLine,
            Column: startColumn,
        }
    }
    
    return Token{
        Type: TokenIdentifier,
        Value: value,
        Line: startLine,
        Column: startColumn,
    }
}

func (l *Lexer) readNumber() Token {
    startPos := l.Position
    startLine := l.Line
    startColumn := l.Column
    
    for l.Position < len(l.Input) {
        char := rune(l.Input[l.Position])
        if !isDigit(char) && char != '.' {
            break
        }
        l.Position++
        l.Column++
    }
    
    value := l.Input[startPos:l.Position]
    
    return Token{
        Type: TokenNumber,
        Value: value,
        Line: startLine,
        Column: startColumn,
    }
}

func (l *Lexer) readString() Token {
    startLine := l.Line
    startColumn := l.Column
    
    // è·³è¿‡å¼€å§‹çš„å¼•å·
    l.Position++
    l.Column++
    
    startPos := l.Position
    
    for l.Position < len(l.Input) {
        char := rune(l.Input[l.Position])
        if char == '"' {
            l.Position++
            l.Column++
            break
        } else if char == '\n' {
            l.Line++
            l.Column = 1
        }
        l.Position++
        l.Column++
    }
    
    value := l.Input[startPos:l.Position-1]
    
    return Token{
        Type: TokenString,
        Value: value,
        Line: startLine,
        Column: startColumn,
    }
}

func (l *Lexer) readOperator() Token {
    startLine := l.Line
    startColumn := l.Column
    
    char := rune(l.Input[l.Position])
    l.Position++
    l.Column++
    
    // æ£€æŸ¥åŒå­—ç¬¦æ“ä½œç¬¦
    if l.Position < len(l.Input) {
        nextChar := rune(l.Input[l.Position])
        if isDoubleOperator(char, nextChar) {
            l.Position++
            l.Column++
            return Token{
                Type: TokenOperator,
                Value: string(char) + string(nextChar),
                Line: startLine,
                Column: startColumn,
            }
        }
    }
    
    return Token{
        Type: TokenOperator,
        Value: string(char),
        Line: startLine,
        Column: startColumn,
    }
}

func (l *Lexer) readDelimiter() Token {
    startLine := l.Line
    startColumn := l.Column
    
    char := rune(l.Input[l.Position])
    l.Position++
    l.Column++
    
    return Token{
        Type: TokenDelimiter,
        Value: string(char),
        Line: startLine,
        Column: startColumn,
    }
}
```

### 4.2 è¾…åŠ©å‡½æ•°

```go
func isLetter(char rune) bool {
    return (char >= 'a' && char <= 'z') || (char >= 'A' && char <= 'Z')
}

func isDigit(char rune) bool {
    return char >= '0' && char <= '9'
}

func isOperator(char rune) bool {
    operators := []rune{'+', '-', '*', '/', '=', '<', '>', '!', '&', '|', '^', '%'}
    for _, op := range operators {
        if char == op {
            return true
        }
    }
    return false
}

func isDelimiter(char rune) bool {
    delimiters := []rune{'(', ')', '[', ']', '{', '}', ';', ',', '.', ':'}
    for _, delim := range delimiters {
        if char == delim {
            return true
        }
    }
    return false
}

func isKeyword(value string) bool {
    keywords := []string{
        "func", "var", "const", "type", "struct", "interface",
        "if", "else", "for", "range", "switch", "case", "default",
        "return", "break", "continue", "defer", "go", "select",
        "chan", "map", "package", "import",
    }
    
    for _, keyword := range keywords {
        if value == keyword {
            return true
        }
    }
    return false
}

func isDoubleOperator(first, second rune) bool {
    doubleOps := map[rune][]rune{
        '=': {'='},
        '!': {'='},
        '<': {'=', '<'},
        '>': {'=', '>'},
        '&': {'&'},
        '|': {'|'},
        '+': {'+', '='},
        '-': {'-', '='},
        '*': {'='},
        '/': {'=', '/'},
        '%': {'='},
        '^': {'='},
    }
    
    if ops, exists := doubleOps[first]; exists {
        for _, op := range ops {
            if second == op {
                return true
            }
        }
    }
    return false
}
```

## 5. Goè¯­è¨€å®ç°

### 5.1 å®Œæ•´çš„è¯æ³•åˆ†æå™¨

```go
// GoLexer Goè¯­è¨€è¯æ³•åˆ†æå™¨
type GoLexer struct {
    *Lexer
    Keywords map[string]bool
    Operators map[string]bool
}

func NewGoLexer(input string) *GoLexer {
    lexer := &GoLexer{
        Lexer: &Lexer{
            Input: input,
            Position: 0,
            Line: 1,
            Column: 1,
        },
        Keywords: make(map[string]bool),
        Operators: make(map[string]bool),
    }
    
    // åˆå§‹åŒ–Goè¯­è¨€å…³é”®å­—
    goKeywords := []string{
        "break", "case", "chan", "const", "continue", "default", "defer",
        "else", "fallthrough", "for", "func", "go", "goto", "if", "import",
        "interface", "map", "package", "range", "return", "select", "struct",
        "switch", "type", "var",
    }
    
    for _, keyword := range goKeywords {
        lexer.Keywords[keyword] = true
    }
    
    // åˆå§‹åŒ–æ“ä½œç¬¦
    goOperators := []string{
        "+", "-", "*", "/", "%", "&", "|", "^", "<<", ">>", "&^",
        "+=", "-=", "*=", "/=", "%=", "&=", "|=", "^=", "<<=", ">>=", "&^=",
        "&&", "||", "<-", "++", "--", "==", "<", ">", "!=", "<=", ">=",
        "=", ":=", "...",
    }
    
    for _, op := range goOperators {
        lexer.Operators[op] = true
    }
    
    return lexer
}

func (gl *GoLexer) Tokenize() []Token {
    return gl.Lexer.Tokenize()
}
```

## 6. åº”ç”¨å®ä¾‹

### 6.1 ç®€å•è¡¨è¾¾å¼è¯æ³•åˆ†æ

```go
func ExampleExpressionLexing() {
    input := "x = 42 + y * 3"
    
    lexer := NewGoLexer(input)
    tokens := lexer.Tokenize()
    
    for _, token := range tokens {
        fmt.Printf("Token: %s, Type: %d, Line: %d, Column: %d\n",
            token.Value, token.Type, token.Line, token.Column)
    }
}
```

### 6.2 Goä»£ç è¯æ³•åˆ†æ

```go
func ExampleGoCodeLexing() {
    input := `
package main

import "fmt"

func main() {
    x := 42
    fmt.Println("Hello, World!")
}
`
    
    lexer := NewGoLexer(input)
    tokens := lexer.Tokenize()
    
    for _, token := range tokens {
        if token.Type != TokenEOF {
            fmt.Printf("%s ", token.Value)
        }
    }
    fmt.Println()
}
```

## æ€»ç»“

è¯æ³•åˆ†ææ˜¯ç¼–è¯‘å™¨çš„ç¬¬ä¸€ä¸ªé˜¶æ®µï¼Œè´Ÿè´£å°†æºä»£ç å­—ç¬¦æµè½¬æ¢ä¸ºè¯æ³•å•å…ƒåºåˆ—ã€‚é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼å’Œæœ‰é™è‡ªåŠ¨æœºçš„ç†è®ºåŸºç¡€ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºé«˜æ•ˆå‡†ç¡®çš„è¯æ³•åˆ†æå™¨ã€‚

**å…³é”®è¦ç‚¹**ï¼š

1. **ç†è®ºåŸºç¡€**ï¼šåŸºäºæ­£åˆ™è¡¨è¾¾å¼å’Œæœ‰é™è‡ªåŠ¨æœºçš„å½¢å¼åŒ–ç†è®º
2. **å®ç°æŠ€æœ¯**ï¼šDFAå’ŒNFAçš„è½¬æ¢å’Œä¼˜åŒ–
3. **å®é™…åº”ç”¨**ï¼šåœ¨ç¼–è¯‘å™¨ã€è§£é‡Šå™¨ã€æ–‡æœ¬å¤„ç†ç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨
4. **Goè¯­è¨€ç‰¹æ€§**ï¼šå……åˆ†åˆ©ç”¨Goè¯­è¨€çš„å­—ç¬¦ä¸²å¤„ç†å’Œmapæ•°æ®ç»“æ„

**æ¿€æƒ…æ¾æ¹ƒçš„æŒç»­æ„å»º** <(ï¿£ï¸¶ï¿£)â†—[GO!] **è¯æ³•åˆ†æç†è®ºå®Œæˆï¼** ğŸš€
